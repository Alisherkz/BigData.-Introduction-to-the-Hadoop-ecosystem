Виды получения данных

ETL:
1. Extract
2. Transform
3. Load

Batch:
	T-1 - раз в день выгружает
	Micro-batch - раз в час выгружает

ELT
1. Extract
2. Load
3. Transform

Streaming:
	NRT (near real time) - раз 5 - 10 мин выгружает
	Real time - режим реального времени.

Разница:

--->Разница ETL обозначает извлечение, преобразование и загрузку, в то время как ELT обозначает 
извлечение, загрузку, преобразование.
--->ETL загружает данные сначала на промежуточный сервер, а затем в целевую систему, тогда как ELT загружает 
данные непосредственно в целевую систему.
--->Модель ETL используется для локальных, реляционных и структурированных данных, в то время как ELT используется для 
масштабируемых облачных структурированных и неструктурированных источников данных.
--->ETL в основном используется для небольшого количества данных, тогда как ELT используется для больших объемов данных.
--->ETL не обеспечивает поддержку озера данных, в то время как ELT обеспечивает поддержку озера данных.
--->ETL легко внедрить, в то время как ELT требует нишевых навыков для внедрения и поддержки.

ETL: Легко в управлении, но после сохранение сложно будет добавить новыу колонку.

ELT: Легко будет управлять после сохранение, но так как ELT сохраняет все подрядт может загружать долго надо дополнить.



Инструменты
1. HDFS (hdfs dfs -get)			
2. Apache SQOOP	- позволяет выгружать данные из реляционных таблиц и сохранить в hdfs
3. ODBC/JDBC Connectors	- мы можем написать скрипт который позволяет приобразовывать и сохранить данные в hdfs
4. Informatica -
5. Oracle Data Integrator -
6. Apache NiFi -
7. Apache Flume - хорошо работает в выгрузках
8. Apache Spark - построение модели или граф и т.д.
9. Apache Flink -
10. Apache Kafka - мы можем построет отказоустойчивую систему
11. Apache Storm - 
12. Apache Samza -


		SQOOP

SQOOP Имеет Импорт и Экспорт. SQOOP - мы указываем запрос, что он должен подключится какой-то баз данных, после чего он
начинает позновать, что и какая это тип данных и какие колонкий имеет. И на основе этого SQOOP генерирует запрос Hive-у 
на создание нужный таблицы. То есть с помощью SQOOP мы создаем таблицу в Hive и в HiveMetaStore сохраняется метаданные.
И с помощью SQOOP создается контейнеры который берут и разделяет на части наш датасет и запускает каждый в ноде процесс по 
выгрузке данных, то есть SQOOP загружает данные в HDFS, таким образом SQOOP позволяет быстро выгружать данные.


		ODBC/JDBC Connectors

Мы можем написать подключится к какой то базе данных у которого есть CONNECTOR но медленно выгружает данные.
Универсиально мы можем подключится к любой базе.

		Apache Flume

Предназначение Apache Flume быстро с Веб сервера создать источник данных создает канал или сеть и сохраняет в HDFS

		Apache Spark & Apache Flink

Apache Spark Умеет все, но с небольшой задержкой, c Apache Spark можно преобразовать данные можно провести широкий 
анализ данных.
Apache Flink тот же Spark, только быстрее, Минусы умалчиваются.

		Apache Kafka

Благодаря Kafka мы можем соединить несколько источников, может создать и вычитовать данных, и преобразует их.
Мы не теряем данные, Kafka может дублировать данные. Используется в высоконагруженных системах